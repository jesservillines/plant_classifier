{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "671c318c-cb29-4914-ab9a-5601635e1dfe"
    }
   },
   "source": [
    "# Creating our own Image Classifier using Amazon Sagemaker\n",
    "\n",
    "This is the hands-on Jupyter notebook of the article with the same name, published on the AWS AI Blog. The idea here is to show you how to create an end-to-end Image Classifcation solution, using Amazon Sagemaker. We will use here a technique called Transfer Learning, detailed in the blog post. We will pick a pre-trained Resnet152 (Imagenet11K) and specialize (re-train) it to classify 10 different pieces of clothing and accessories.\n",
    "\n",
    "This hands on is comprised in four parts:\n",
    "\n",
    "   1. [Preparing your dataset](#Preparing-your-dataset)\n",
    "       1. [Download the files (idx format)](#Let's-start-by-downloading-the-Fashin-MNIST-dataset)\n",
    "       1. [Extract all the images](#Now-let's-create-a-directories-structure-for-our-images)\n",
    "       1. [Create the RecordIO .lst files](#Creating-the-RecordIO-list-files)\n",
    "       1. [Create the RecordIO .rec files](#Then,-using-the-.lst-files,-let's-create-both-RecordIO-files-%28train-and-test%29)\n",
    "       1. [Upload the prepared dataset to S3](#Good.-Now,-let's-upload-the-.rec-files-to-S3)\n",
    "   2. [Setup your Environment](#Setup-your-Environment)\n",
    "   3. [Create your model](#Create-your-model)\n",
    "       3. Train your model\n",
    "       3. Pack your model\n",
    "       3. Create an Endpoint Configuration\n",
    "       3. Deploy your model\n",
    "   4. [Test your model](#Test-your-model)\n",
    "\n",
    "----    \n",
    "This notebooks is based on the original Amazon Sagemaker sample notebook: https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/imageclassification_caltech/Image-classification-transfer-learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fc1aa581-f64f-4097-8e21-a057eaa9b220"
    }
   },
   "source": [
    "## Preparing your dataset\n",
    "For this experiment we will use a famous public dataset called Fashion MNIST. So, the outcome of this process is a trained model capable of classifying all the categories from this dataset.\n",
    "\n",
    "### Fashion MNIST\n",
    "Fashion-MNIST is a dataset of Zalando’s article, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28×28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "#### CLASSES\n",
    "\n",
    "\n",
    "  1. T-shirt/top\n",
    "  2. Trouser\n",
    "  3. Pullover\n",
    "  4. Dress\n",
    "  5. Coat\n",
    "  6. Sandal\n",
    "  7. Shirt\n",
    "  8. Sneaker\n",
    "  9. Bag\n",
    "  10. Ankle boot\n",
    "\n",
    "#### References\n",
    "\n",
    "https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/\n",
    "\n",
    "https://github.com/zalandoresearch/fashion-mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='/tmp/fashion'\n",
    "\n",
    "resources_base_url='https://s3.amazonaws.com/aws-ml-blog/artifacts/image-classification/fashion-mnist'\n",
    "test_images=resources_base_url + '/test_images.zip'\n",
    "pre_trained_model=resources_base_url + '/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0e75a7bf-66b7-4a9a-96bb-3927f15654f5"
    }
   },
   "source": [
    "### Let's start by downloading the Fashin MNIST dataset\n",
    "Please notice that it is in **idx** format. Given the Amazon Sagemaker built-in algorithm for [Image Classification](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html) expects as input a dataset formated in [RecordIO](https://mxnet.incubator.apache.org/faq/recordio.html), we need to extract all the images from it and prepare the final [RecordIO](https://mxnet.incubator.apache.org/faq/recordio.html) files (train & test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "23227f71-5a9c-4066-8374-8bbd5dba3bc4"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p $base_dir/samples\n",
    "!curl http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz | gunzip > $base_dir/samples/train-images-idx3-ubyte\n",
    "!curl http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz | gunzip > $base_dir/samples/train-labels-idx1-ubyte\n",
    "!curl http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz | gunzip > $base_dir/samples/t10k-images-idx3-ubyte\n",
    "!curl http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz | gunzip > $base_dir/samples/t10k-labels-idx1-ubyte\n",
    "!ls -lat $base_dir/samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "561d7aef-9803-4dcb-8ac6-4c864673b737"
    }
   },
   "source": [
    "### Now let's create a directories structure for our images\n",
    "All the images of a particular class will be stored in its respective directory. Since Fashin MNIST has 10 distinct classes, we'll have 10 different directories, one for each class.\n",
    "  \n",
    "**DIRECTORY STRUCTURE**\n",
    "  - ./fashion_mnist/\n",
    "      - TShirtTop/image1.jpg\n",
    "      - TShirtTop/image2.jpg\n",
    "      - Trouser/image100.jpg\n",
    "      - ..\n",
    "      - AnkleBoot/imageN.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e120c3a0-ba53-45d5-a45e-5e0caa407563"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir -p $base_dir/fashion_mnist\n",
    "\n",
    "import os\n",
    "categories = ['TShirtTop', 'Trouser', 'Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','AnkleBoot' ]\n",
    "\n",
    "for i in categories:\n",
    "    try:\n",
    "        os.mkdir(base_dir + '/fashion_mnist/%s' % i)\n",
    "    except OSError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e548ba7a-4550-4d2f-8c1f-d3de28f3b4da"
    }
   },
   "source": [
    "### Then, let's unpack the dataset to the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1fe6d333-5a1b-412b-9808-9fa4f11db28f"
    }
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "mndata = MNIST(base_dir + '/samples')\n",
    "counter = 0\n",
    "images, labels = mndata.load_training()\n",
    "for i, img in enumerate(images):\n",
    "    img = np.reshape(img, (28, 28))\n",
    "    img = Image.fromarray(np.uint8(np.array(img)))\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(base_dir + '/fashion_mnist/%s/img_%d.jpg' % (categories[labels[i]], counter ))\n",
    "    counter += 1\n",
    "\n",
    "images, labels = mndata.load_testing()\n",
    "for i, img in enumerate(images):\n",
    "    img = np.reshape(img, (28, 28))\n",
    "    img = Image.fromarray(np.uint8(np.array(img)))\n",
    "    img = img.convert(\"RGB\")\n",
    "    img.save(base_dir + '/fashion_mnist/%s/img_%d.jpg' % (categories[labels[i]], counter ))\n",
    "    counter += 1\n",
    "\n",
    "!ls -lat $base_dir/fashion_mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c36deb51-de14-407e-9642-17d86ab58d2c"
    }
   },
   "source": [
    "### Creating the RecordIO list files\n",
    "Right, now that we have all the images in their respective directories, one per class, it is time to create the RecordIO file. RecordIO is a optimized file format that will feed our images to the Neural Network during training.\n",
    "\n",
    "We will split the dataset into training (70%) and testing (30%). To do that, we'll run a python script (im2rec), which is the best tool for this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0982a9e8-4158-411f-a498-2e96b3de2ed7"
    }
   },
   "outputs": [],
   "source": [
    "# Here we will search for the python script im2rec\n",
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "%env IM2REC=$im2rec\n",
    "%env BASE_DIR=$base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8f5f90f6-948a-4422-9956-ea0fe4ed867a"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Ok. Here, im2rec will read all the images and create two .lst files, one for training and other for validation\n",
    "# this files will then be used for creating the RecordIO files\n",
    "\n",
    "cd $BASE_DIR\n",
    "python $IM2REC --list --recursive --test-ratio=0.3 --train-ratio=0.7 fashion_mnist fashion_mnist/\n",
    "ls *.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "154bd29c-036c-4f23-803a-8d64955f8f79"
    }
   },
   "source": [
    "###  Then, using the .lst files, let's create both RecordIO files (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ea78eb4f-ec40-4fdf-abfd-85447db56d56"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "cd $BASE_DIR\n",
    "python $IM2REC --num-thread=4 --pass-through fashion_mnist_train.lst fashion_mnist\n",
    "python $IM2REC --num-thread=4 --pass-through fashion_mnist_test.lst fashion_mnist\n",
    "ls *.rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fe5c926-df20-47c0-bcf8-f0263aef81dd"
    }
   },
   "source": [
    "### Good. Now, let's upload the .rec files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "3b2920dc-d36c-4f66-9a39-2a844b6d8056"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get the current Sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket=sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = sagemaker_session.upload_data(path=base_dir + '/fashion_mnist_train.rec', key_prefix='fashion-mnist/train')\n",
    "test_path = sagemaker_session.upload_data(path=base_dir + '/fashion_mnist_test.rec', key_prefix='fashion-mnist/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "199a1226-2c16-456c-96d6-24085aa9d6ba"
    }
   },
   "source": [
    "## Setup your Environment\n",
    "\n",
    "Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are three parts to this:\n",
    "\n",
    "   1. The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "   2. The S3 bucket that you want to use for training and model data\n",
    "   3. The Amazon sagemaker image classification docker image which need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e4d5b214-4329-4e4d-ad8a-5fe80edd73a4"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# 1. Obtaining the role you already configured for Sagemaker when you setup\n",
    "# your Instance notebook (https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "role = get_execution_role()\n",
    "\n",
    "# 2. The S3 Bucket that will store the dataset and the trained model\n",
    "# It was already defined above, while we uploaded the RecordIO files to the S3 bucket.\n",
    "\n",
    "# 3. Select the correct Docker image with the Image Classification algorithm\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/image-classification:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/image-classification:latest'}\n",
    "training_image = containers[boto3.Session().region_name]\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3bb42099-2ac3-4648-9a6c-059fe7d53a91"
    }
   },
   "source": [
    "### Here you can find the hyperparameters. These parameters will determine how your model will be trained and, consequently, how your trained model will behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "85ba051d-1bd7-44e9-8347-8d94c6892052"
    }
   },
   "outputs": [],
   "source": [
    "# The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\n",
    "# For this training, we will use 152 layers\n",
    "num_layers = 152\n",
    "# we need to specify the input image shape for the training data\n",
    "image_shape = \"3,28,28\"\n",
    "# we also need to specify the number of training samples in the training set\n",
    "# for fashion_mnist it is 70012\n",
    "num_training_samples = 70012\n",
    "# specify the number of output classes\n",
    "num_classes = 10\n",
    "# batch size for training\n",
    "mini_batch_size = 1024\n",
    "# number of epochs\n",
    "epochs = 40\n",
    "# learning rate\n",
    "learning_rate = 0.00001\n",
    "# Since we are using transfer learning, we set use_pretrained_model to 1 so that weights can be \n",
    "# initialized with pre-trained weights\n",
    "use_pretrained_model = 1\n",
    "# Training algorithm/optimizer. Default is SGD\n",
    "optimizer = 'sgd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c4c0fb1b-7a6b-4232-904f-43ff000463a5"
    }
   },
   "source": [
    "### Using all the parameters and hyperparameters we just defined, let's wrap up in an structure for Sagemaker to start a job for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "34cc3c7a-ae19-45cb-8d8e-de0316630db3"
    }
   },
   "outputs": [],
   "source": [
    "dataset_prefix='fashion-mnist'\n",
    "# create unique job name \n",
    "job_name_prefix = 'fashion-mnist'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = role\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": 's3://{}/{}/output'.format(bucket, job_name_prefix)\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.p2.xlarge\",\n",
    "    \"VolumeSizeInGB\": 50\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = {\n",
    "    \"image_shape\": image_shape,\n",
    "    \"num_layers\": str(num_layers),\n",
    "    \"num_training_samples\": str(num_training_samples),\n",
    "    \"num_classes\": str(num_classes),\n",
    "    \"mini_batch_size\": str(mini_batch_size),\n",
    "    \"epochs\": str(epochs),\n",
    "    \"learning_rate\": str(learning_rate),\n",
    "    \"use_pretrained_model\": str(use_pretrained_model),\n",
    "    \"optimizer\": optimizer\n",
    "}\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = []\n",
    "\n",
    "# Please notice that we're using application/x-recordio for both \n",
    "# training and validation datasets, given our dataset is formated in RecordIO\n",
    "\n",
    "# Here we set training dataset\n",
    "# Training data should be inside a subdirectory called \"train\"\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"train\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/train/'.format(bucket, dataset_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "# Here we set validation dataset\n",
    "# Validation data should be inside a subdirectory called \"validation\"\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/test/'.format(bucket, dataset_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model\n",
    "You'll create your model in four steps:\n",
    "  1. First you'll submit a job for sagemaker to train your model using your dataset uploaded to S3\n",
    "  2. Second you'll pack the job output into model ready to be used\n",
    "  3. Then you'll create an Endpoint Configuration, which is the metadata used by Sagemaker to deploy your model\n",
    "  4. Finally you'll deploy your model using the Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "27b959cc-bad2-436a-8f66-5f7afce589c8"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Sagemaker client\n",
    "sagemaker = boto3.client(service_name='sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1/4 - Train your model\n",
    "### --> If you just want to deploy a pre-trained model to see how this code works, you can skip this part.\n",
    "\n",
    "Here we will submit a job, described by the training parameters defined above, for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e2720fcb-1faf-4e53-8b56-8fddf927be65"
    }
   },
   "outputs": [],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2/4 - Pack your model\n",
    "### --> Set use_pretrained_model=True if you skipped the last part\n",
    "Ok. It is time to convert the job output into a model.\n",
    "After running the commend bellow it will appear on the Model section of your Sagemaker console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1ffc5b9a-bcac-4325-93ec-8eb6a1452ca6"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "use_pretrained_model=False\n",
    "\n",
    "model_name=\"fashion-mnist\" + time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "print(model_name)\n",
    "if use_pretrained_model:\n",
    "    prefix=\"fashion-mnist/model/model.tar.gz\"\n",
    "    model_data=\"s3://{}/{}\".format(bucket, prefix)\n",
    "    s3 = boto3.client('s3')\n",
    "    resp = s3.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "    if resp.get(\"Contents\") is None:\n",
    "        print(\"Please wait. It will take around 6mins\")\n",
    "        !curl -s $pre_trained_model | aws s3 cp - s3://$bucket/fashion-mnist/model/model.tar.gz\n",
    "else:\n",
    "    info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "    print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_model_response = sagemaker.create_model(\n",
    "        ModelName = model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        PrimaryContainer = primary_container)\n",
    "    print(create_model_response['ModelArn'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3/4 Create an Endpoint Config for your model\n",
    "At launch, we will support configuring REST endpoints in hosting with multiple models, e.g. for A/B testing purposes. In order to support this, customers create an endpoint configuration, that describes the distribution of traffic across the models, whether split, shadowed, or sampled in some way.\n",
    "\n",
    "In addition, the endpoint configuration describes the instance type required for model deployment, and at launch will describe the autoscaling configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dd2012d1-bb70-43e3-bd28-cad07f538a5c"
    }
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-epc-' + timestamp\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.c4.2xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4/4 - Deploy your model\n",
    "\n",
    "Lastly, you will create the endpoint that serves up the model, through specifying the name and configuration defined above. The end result is an endpoint that can be validated and incorporated into production applications. This takes 9-11 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "57e021bd-974e-4ec3-96a9-a9335ce5a2eb"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your model\n",
    "\n",
    "Let's recapitulate. We've just trained an Image Classifier model using Fashion MNIST. What does that mean? It means that now, we have an 'WebService' accessible by the endpoint we just deployed, that has the power to classify 10 different types of objects: 1) Ankle Boot; 2) Bag; 3) Coat; 4) Dress; 5) Pullover; 6) Sandal; 7) Shirt; 8) Sneaker; 9) TShirt 10) Trouser.\n",
    "\n",
    "With that in mind let's test our model in some examples and see what happens. If you decided to use the pre-trained model in step 2/4, you'll see that the model is capable of making good predictions for the test images.\n",
    "\n",
    "In **test_data** directory you will find 5 images of 5 real itens as you can see bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s $test_images -o /tmp/test_images.zip\n",
    "!mkdir $base_dir/test_data\n",
    "!unzip /tmp/test_images.zip -d $base_dir/test_data\n",
    "!rm -f /tmp/test_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "test_categories = ['Shirt','TShirtTop', 'AnkleBoot', 'Sneaker', 'Bag']\n",
    "\n",
    "f, axarr = plt.subplots(1, 5, figsize=(20,12))\n",
    "col = 0\n",
    "for i in range(5):\n",
    "    im = Image.open(base_dir + '/test_data/item%d_thumb.jpg' % (i+1))\n",
    "    axarr[col].text(0, 0, '%s' %(test_categories[i] ), fontsize=15, color='blue')\n",
    "    frame = axarr[col].imshow(im)\n",
    "    col += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, let's do some predictions with the images above! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "runtime = boto3.Session().client(service_name='sagemaker-runtime') \n",
    "object_categories = ['AnkleBoot','Bag','Coat','Dress','Pullover','Sandal','Shirt','Sneaker','TShirtTop','Trouser']\n",
    "\n",
    "_, axarr = plt.subplots(1, 5, figsize=(20,12))\n",
    "col = 0\n",
    "for i in range(5):\n",
    "    \n",
    "    # Load the image bytes\n",
    "    img = open(base_dir + '/test_data/item%d_thumb.jpg' % (i+1), 'rb').read()\n",
    "    \n",
    "    # Call your model for predicting which object appears in this image.\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType='application/x-image', \n",
    "        Body=bytearray(img)\n",
    "    )\n",
    "    # read the prediction result and parse the json\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    # which category has the highest confidence?\n",
    "    pred_label_id = np.argmax(result)\n",
    "    \n",
    "    # Green when our model predicted correctly, otherwise, Red\n",
    "    text_color = 'red'\n",
    "    if object_categories[pred_label_id] == test_categories[i]:\n",
    "        text_color = 'green'\n",
    "\n",
    "    # Render the text for each image/prediction\n",
    "    output_text = '%s (%f)' %(object_categories[pred_label_id], result[pred_label_id] )\n",
    "    axarr[col].text(0, 0, output_text, fontsize=15, color=text_color)\n",
    "    print( output_text )\n",
    "    \n",
    "    # Render the image\n",
    "    img = Image.open(BytesIO(img))\n",
    "    frame = axarr[col].imshow(img)\n",
    "    \n",
    "    col += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yay! We made it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "When we're done with the endpoint, we can just delete it and the backing instances will be released. Run the following cell to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "31a1d98a-c640-4981-97fe-f29a12345648"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker.delete_endpoint(EndpointName=endpoint_name)\n",
    "!rm -rf $base_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
